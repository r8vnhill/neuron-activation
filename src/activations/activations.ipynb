{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Activation Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc256b56b6985559"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "The `activations` module offers a collection of popular activation functions essential for neural network designs.\n",
    "Along with the primary function definitions, this module calculates the gradients for each, aiding in understanding and applying the \n",
    "backpropagation algorithm.\n",
    "Notably, the `softmax` function is an exception due to its inherent multi-input, multi-output structure, necessitating a unique \n",
    "gradient computation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "878e5cb4dd4620ed"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path('..') / '..'\n",
    "!pip install -q -r {ROOT_DIR / 'requirements.txt'}\n",
    "\n",
    "import torch  # needed for running the examples\n",
    "from tqdm import tqdm  # prettier progress bars\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:06.597962800Z",
     "start_time": "2023-08-19T03:03:04.687499600Z"
    }
   },
   "id": "23c1d9f7a1e0e6b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's an improved and restructured version:\n",
    "\n",
    "## Sigmoid Activation Function\n",
    "\n",
    "The sigmoid function is a type of activation function that is primarily used in binary \n",
    "classification tasks.\n",
    "It maps any input to a value between 0 and 1, which can often be used to represent the \n",
    "probability that a given input point belongs to the positive class.\n",
    "\n",
    "Mathematically, the sigmoid function is given by:\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Its derivative, crucial for the backpropagation algorithm, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}'(x) = \\mathrm{sigmoid}(x)(1 - \\mathrm{sigmoid}(x))\n",
    "$$\n",
    "\n",
    "However, it's worth noting that the sigmoid function can lead to vanishing gradients when its \n",
    "input is very high or very low."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f0fc4f679400c65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cffb92aec174e420"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Computing the sigmoid of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56fb9b7e9ac1e0be"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 0.7311, 0.8808])\n"
     ]
    }
   ],
   "source": [
    "from activations import sigmoid\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "result = sigmoid(x)\n",
    "print(result)  # Outputs: tensor([0.5000, 0.7311, 0.8808])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:06.604963500Z",
     "start_time": "2023-08-19T03:03:06.598963600Z"
    }
   },
   "id": "3c89c56b254a8309"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Determining the gradient of the sigmoid for a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0553e3d7d83a3b"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.1966, 0.1050])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([0, 1, 2])\n",
    "gradient_result = sigmoid(x, gradient=True)\n",
    "print(gradient_result)  # Outputs: tensor([0.2500, 0.1966, 0.1050])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:06.618108500Z",
     "start_time": "2023-08-19T03:03:06.602965200Z"
    }
   },
   "id": "b22d9f3a9c8e1f02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Handling higher-dimensional tensors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2808ab4b7ec08a0c"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.7311],\n",
      "        [0.2689, 0.8808]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[0, 1], [-1, 2]])\n",
    "result = sigmoid(x)\n",
    "print(result)\n",
    "# Outputs: \n",
    "# tensor([[0.5000, 0.7311],\n",
    "#         [0.2689, 0.8808]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:06.680670200Z",
     "start_time": "2023-08-19T03:03:06.615450200Z"
    }
   },
   "id": "97423d9af3f56da6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Verifying against PyTorch's built-in implementation:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ae0e5ce57b57abf"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 40.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    our_implementation = sigmoid(x)\n",
    "    pytorch_implementation = torch.sigmoid(x)\n",
    "    assert torch.allclose(our_implementation, pytorch_implementation), \\\n",
    "        f\"Expected {pytorch_implementation}, but got {our_implementation}\"\n",
    "print(\"All tests passed!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:09.108186700Z",
     "start_time": "2023-08-19T03:03:06.624154800Z"
    }
   },
   "id": "fe880629375d6e35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tanh\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd59f4a01c505151"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tanh Activation Function\n",
    "\n",
    "The hyperbolic tangent, or simply $\\text{tanh}$, is another prevalent activation function used\n",
    "in neural networks.\n",
    "Its outputs range between -1 and 1, making it zero-centered, which can help mitigate some of\n",
    "the issues observed with non-zero-centered activation functions like the sigmoid.\n",
    "\n",
    "Mathematically, the $\\text{tanh}$ function is expressed as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Or, equivalently, as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}(x) = 2 \\times \\mathrm{sigmoid}(2x) - 1\n",
    "$$\n",
    "\n",
    "The derivative of $\\text{tanh}$, useful for backpropagation, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}'(x) = 1 - \\mathrm{tanh}^2(x)\n",
    "$$\n",
    "\n",
    "Compared to the sigmoid function, $\\text{tanh}$ tends to be preferred for hidden layers due to\n",
    "its zero-centered nature.\n",
    "Still, it shares the vanishing gradient problem for extremely high or low inputs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bc8276f95ed74ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Examples\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c5ad6063af7aaa2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 1. Computing the $\\text{tanh}$ of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eed752b50ab3e51"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.7616, 0.9640])\n"
     ]
    }
   ],
   "source": [
    "from activations import tanh\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "result = tanh(x)\n",
    "print(result)  # Expected: tensor([0.0000, 0.7616, 0.9640])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:09.109187800Z",
     "start_time": "2023-08-19T03:03:09.104187800Z"
    }
   },
   "id": "522102d9eaf59cef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Determining the gradient of $\\text{tanh}$ for a tensor:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e513fafb20c86e2"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.4200, 0.0707])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([0, 1, 2])\n",
    "gradient_result = tanh(x, gradient=True)\n",
    "print(gradient_result)  # Expected: tensor([1.0000, 0.4200, 0.0707])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:09.119186800Z",
     "start_time": "2023-08-19T03:03:09.109187800Z"
    }
   },
   "id": "e067d404338042d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Handling higher-dimensional tensors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c642d18ce87c711e"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.7616],\n",
      "        [-0.7616,  0.9640]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[0, 1], [-1, 2]])\n",
    "result = tanh(x)\n",
    "print(result)\n",
    "# Expected: \n",
    "# tensor([[ 0.0000,  0.7616],\n",
    "#         [-0.7616,  0.9640]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:09.190766100Z",
     "start_time": "2023-08-19T03:03:09.117188200Z"
    }
   },
   "id": "eb2332ae6062c31c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Verifying against PyTorch's built-in implementation:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f836e1f7682a0b2"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    actual, expected = tanh(x), torch.tanh(x)\n",
    "    assert torch.allclose(actual, expected, atol=1e-7), f\"Expected {expected}, but got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:03:11.748249900Z",
     "start_time": "2023-08-19T03:03:09.124701500Z"
    }
   },
   "id": "c0bb1fd44e6feca6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU\n",
    "\n",
    "**Location**: [`relu.py`](relu.py)\n",
    "\n",
    "**Formula**: \n",
    "\n",
    "![](https://quicklatex.com/cache3/a0/ql_37999e1feff124baca2413a754b3bfa0_l3.png)\n",
    "\n",
    "*Description*: ReLU imparts non-linearity in models without perturbing the receptive fields of convolutions.\n",
    "\n",
    "**Gradient**: \n",
    "\n",
    "![](https://quicklatex.com/cache3/41/ql_f90d9e252e775d7a7c0591082d1fd941_l3.png)\n",
    "\n",
    "Being computationally efficient, ReLU can occasionally cause dead neurons during the training process.\n",
    "\n",
    "## Swish\n",
    "\n",
    "**Location**: [`swish.py`](swish.py)\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "![](https://quicklatex.com/cache3/8d/ql_70016b335ea865d9640e6af078f4e08d_l3.png)\n",
    "\n",
    "Where: ![](https://quicklatex.com/cache3/e8/ql_9a315236dfcda864a869107144a3fbe8_l3.png) is a learnable parameter.\n",
    "\n",
    "*Description*: Swish stands as a self-gated function, synthesizing the merits of both ReLU and sigmoid.\n",
    "\n",
    "**Gradient**: \n",
    "\n",
    "![](https://quicklatex.com/cache3/00/ql_e0b16a0e5c70dc33fae2dd6df8f09400_l3.png)\n",
    "\n",
    "## CELU\n",
    "\n",
    "**Location**: [`celu.py`](celu.py)\n",
    "\n",
    "**Formula**: \n",
    "\n",
    "![](https://quicklatex.com/cache3/f3/ql_c5273c8c3683571ded65e128719665f3_l3.png)\n",
    "\n",
    "*Description*: CELU is an extension of the exponential linear units (ELU), enhanced with a scalable parameter \n",
    "![](https://quicklatex.com/cache3/a0/ql_0c3e2deb84c57937afcc3a11a786fea0_l3.png)\n",
    "\n",
    "**Gradient**:\n",
    "\n",
    "![](https://quicklatex.com/cache3/a9/ql_f5f8b0d44fbd0efab3c215f8bf8ea6a9_l3.png)\n",
    "\n",
    "## Softmax\n",
    "\n",
    "**Location**: [`softmax.py`](softmax.py)\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "![](https://quicklatex.com/cache3/59/ql_9798671b5f273c3282d12bd273d73b59_l3.png)\n",
    "\n",
    "*Description*: The softmax function is essential for multi-class categorization tasks, transmuting inputs into a\n",
    "probability distribution spread across multiple categories.\n",
    "\n",
    "**Gradient**: Evaluating the gradient of softmax demands intricate attention due to the inherent normalization.\n",
    "This computation typically necessitates the use of the _Jacobian matrix_.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fae75a489d5540f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
