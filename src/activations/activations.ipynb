{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Activation Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc256b56b6985559"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "The `activations` module offers a collection of popular activation functions essential for neural network designs.\n",
    "Along with the primary function definitions, this module calculates the gradients for each, aiding in understanding and applying the \n",
    "backpropagation algorithm.\n",
    "Notably, the `softmax` function is an exception due to its inherent multi-input, multi-output structure, necessitating a unique \n",
    "gradient computation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "878e5cb4dd4620ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path('..') / '..'\n",
    "!pip install -q -r {ROOT_DIR / 'requirements.txt'}\n",
    "\n",
    "import torch  # needed for running the examples\n",
    "from tqdm import tqdm  # prettier progress bars\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23c1d9f7a1e0e6b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Sigmoid\n",
    "\n",
    "The sigmoid function confines its input within the range of 0 and 1.\n",
    "It is commonly used in binary classification tasks, where the output is interpreted as the probability of the input\n",
    "belonging to the positive class.\n",
    "\n",
    "Formally, the sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "And its gradient is:\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}'(x) = \\mathrm{sigmoid}(x)(1 - \\mathrm{sigmoid}(x))\n",
    "$$\n",
    "\n",
    "Notably, this gradient can induce the vanishing gradient issue for extremely high or low values.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f0fc4f679400c65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cffb92aec174e420"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 1: Compute the sigmoid of a tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56fb9b7e9ac1e0be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from activations import sigmoid\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "result = sigmoid(x)\n",
    "print(result)  # tensor([0.5000, 0.7311, 0.8808])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c89c56b254a8309"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 2: Compute the gradient of the sigmoid of a tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0553e3d7d83a3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([0, 1, 2])\n",
    "result_with_gradient = sigmoid(x, gradient=True)\n",
    "print(result_with_gradient)  # tensor([0.2500, 0.1966, 0.1050])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b22d9f3a9c8e1f02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 3: Higher-dimensional tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2808ab4b7ec08a0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([[0, 1], [-1, 2]])\n",
    "result = sigmoid(x)\n",
    "print(result)\n",
    "# tensor([[0.5000, 0.7311],\n",
    "#         [0.2689, 0.8808]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97423d9af3f56da6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 4: Testing against PyTorch's implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ae0e5ce57b57abf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    actual, expected = sigmoid(x), torch.sigmoid(x)\n",
    "    assert torch.allclose(actual, expected), f\"Expected {expected}, got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe880629375d6e35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tanh\n",
    "\n",
    "**Location**: [`tanh.py`](tanh.py)\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "![](https://quicklatex.com/cache3/9f/ql_72eebf5038a7f863b236caa209a7b09f_l3.png)\n",
    "\n",
    "*Description*: The tanh function bounds its input within the range of -1 and 1.\n",
    "\n",
    "**Gradient**:\n",
    "\n",
    "![](https://quicklatex.com/cache3/5a/ql_1aec5bd7f300437b4a6a9f9342a6165a_l3.png)\n",
    "\n",
    "## ReLU\n",
    "\n",
    "**Location**: [`relu.py`](relu.py)\n",
    "\n",
    "**Formula**: \n",
    "\n",
    "![](https://quicklatex.com/cache3/a0/ql_37999e1feff124baca2413a754b3bfa0_l3.png)\n",
    "\n",
    "*Description*: ReLU imparts non-linearity in models without perturbing the receptive fields of convolutions.\n",
    "\n",
    "**Gradient**: \n",
    "\n",
    "![](https://quicklatex.com/cache3/41/ql_f90d9e252e775d7a7c0591082d1fd941_l3.png)\n",
    "\n",
    "Being computationally efficient, ReLU can occasionally cause dead neurons during the training process.\n",
    "\n",
    "## Swish\n",
    "\n",
    "**Location**: [`swish.py`](swish.py)\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "![](https://quicklatex.com/cache3/8d/ql_70016b335ea865d9640e6af078f4e08d_l3.png)\n",
    "\n",
    "Where: ![](https://quicklatex.com/cache3/e8/ql_9a315236dfcda864a869107144a3fbe8_l3.png) is a learnable parameter.\n",
    "\n",
    "*Description*: Swish stands as a self-gated function, synthesizing the merits of both ReLU and sigmoid.\n",
    "\n",
    "**Gradient**: \n",
    "\n",
    "![](https://quicklatex.com/cache3/00/ql_e0b16a0e5c70dc33fae2dd6df8f09400_l3.png)\n",
    "\n",
    "## CELU\n",
    "\n",
    "**Location**: [`celu.py`](celu.py)\n",
    "\n",
    "**Formula**: \n",
    "\n",
    "![](https://quicklatex.com/cache3/f3/ql_c5273c8c3683571ded65e128719665f3_l3.png)\n",
    "\n",
    "*Description*: CELU is an extension of the exponential linear units (ELU), enhanced with a scalable parameter \n",
    "![](https://quicklatex.com/cache3/a0/ql_0c3e2deb84c57937afcc3a11a786fea0_l3.png)\n",
    "\n",
    "**Gradient**:\n",
    "\n",
    "![](https://quicklatex.com/cache3/a9/ql_f5f8b0d44fbd0efab3c215f8bf8ea6a9_l3.png)\n",
    "\n",
    "## Softmax\n",
    "\n",
    "**Location**: [`softmax.py`](softmax.py)\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "![](https://quicklatex.com/cache3/59/ql_9798671b5f273c3282d12bd273d73b59_l3.png)\n",
    "\n",
    "*Description*: The softmax function is essential for multi-class categorization tasks, transmuting inputs into a\n",
    "probability distribution spread across multiple categories.\n",
    "\n",
    "**Gradient**: Evaluating the gradient of softmax demands intricate attention due to the inherent normalization.\n",
    "This computation typically necessitates the use of the _Jacobian matrix_.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd59f4a01c505151"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9bc8276f95ed74ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
