{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc256b56b6985559",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e5cb4dd4620ed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "The `activations` module offers a collection of popular activation functions \n",
    "essential for neural network designs.\n",
    "Along with the primary function definitions, this module calculates the \n",
    "gradients for each, aiding in understanding and applying the back-propagation \n",
    "algorithm.\n",
    "Notably, the `softmax` function is an exception due to its inherent multi-input,\n",
    "multi-output structure, necessitating a unique gradient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path('..') / '..'\n",
    "!pip install -q -r {ROOT_DIR / 'requirements.txt'}\n",
    "\n",
    "import torch  # needed for running the examples\n",
    "from tqdm import tqdm  # prettier progress bars\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f731ab65d24aa4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's an improved and restructured version:\n",
    "\n",
    "## Sigmoid Activation Function\n",
    "\n",
    "The sigmoid function is a type of activation function that is primarily used in binary \n",
    "classification tasks.\n",
    "It maps any input to a value between 0 and 1, which can often be used to represent the \n",
    "probability that a given input point belongs to the positive class.\n",
    "\n",
    "Mathematically, the sigmoid function is given by:\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Its derivative, crucial for the backpropagation algorithm, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}'(x) = \\mathrm{sigmoid}(x)(1 - \\mathrm{sigmoid}(x))\n",
    "$$\n",
    "\n",
    "However, it's worth noting that the sigmoid function can lead to vanishing gradients when its \n",
    "input is very high or very low."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7593e414e6ae192e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1367b490e8bc038"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Computing the sigmoid of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b7a374152bd8910"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from activations import sigmoid\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "result = sigmoid(x)\n",
    "print(result)  # Outputs: tensor([0.5000, 0.7311, 0.8808])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0027d14499c660"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Determining the gradient of the sigmoid for a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf9c3112fd10d9e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([0, 1, 2])\n",
    "gradient_result = sigmoid(x, gradient=True)\n",
    "print(gradient_result)  # Outputs: tensor([0.2500, 0.1966, 0.1050])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4a4643ecc84fb99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Handling higher-dimensional tensors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8edd58eda6bb0a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([[0, 1], [-1, 2]])\n",
    "result = sigmoid(x)\n",
    "print(result)\n",
    "# Outputs: \n",
    "# tensor([[0.5000, 0.7311],\n",
    "#         [0.2689, 0.8808]])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71e01266dd43e7c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Verifying against PyTorch's built-in implementation:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9d565802896584e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    our_implementation = sigmoid(x)\n",
    "    pytorch_implementation = torch.sigmoid(x)\n",
    "    assert torch.allclose(our_implementation, pytorch_implementation), \\\n",
    "        f\"Expected {pytorch_implementation}, but got {our_implementation}\"\n",
    "print(\"All tests passed!\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "915387526313a49f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tanh"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63d5430cac08ac3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tanh Activation Function\n",
    "\n",
    "The hyperbolic tangent, or simply $\\text{tanh}$, is another prevalent activation function used\n",
    "in neural networks.\n",
    "Its outputs range between -1 and 1, making it zero-centered, which can help mitigate some of\n",
    "the issues observed with non-zero-centered activation functions like the sigmoid.\n",
    "\n",
    "Mathematically, the $\\text{tanh}$ function is expressed as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Or, equivalently, as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}(x) = 2 \\times \\mathrm{sigmoid}(2x) - 1\n",
    "$$\n",
    "\n",
    "The derivative of $\\text{tanh}$, useful for backpropagation, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}'(x) = 1 - \\mathrm{tanh}^2(x)\n",
    "$$\n",
    "\n",
    "Compared to the sigmoid function, $\\text{tanh}$ tends to be preferred for hidden layers due to\n",
    "its zero-centered nature.\n",
    "Still, it shares the vanishing gradient problem for extremely high or low inputs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a985394154c4e45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Examples\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d4e8e0c651cc57c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 1. Computing the $\\text{tanh}$ of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77cb37f4eed9ab57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from activations import tanh\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "result = tanh(x)\n",
    "print(result)  # Expected: tensor([0.0000, 0.7616, 0.9640])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ba8fb0703a31f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Determining the gradient of $\\text{tanh}$ for a tensor:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ea58baf1ae8f59b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([0, 1, 2])\n",
    "gradient_result = tanh(x, gradient=True)\n",
    "print(gradient_result)  # Expected: tensor([1.0000, 0.4200, 0.0707])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59f9e742846130ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Handling higher-dimensional tensors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d2a3c3964cf3ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([[0, 1], [-1, 2]])\n",
    "result = tanh(x)\n",
    "print(result)\n",
    "# Expected: \n",
    "# tensor([[ 0.0000,  0.7616],\n",
    "#         [-0.7616,  0.9640]])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd3937e5ed106e05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Verifying against PyTorch's built-in implementation:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "273a3801e977fa56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    actual, expected = tanh(x), torch.tanh(x)\n",
    "    assert torch.allclose(actual, expected, atol=1e-7), f\"Expected {expected}, but got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f088d43c476d891"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU (Rectified Linear Unit)\n",
    "\n",
    "ReLU, or Rectified Linear Unit, is one of the most widely used activation functions in deep learning\n",
    "models.\n",
    "It is especially popular in convolutional neural networks and deep feed-forward networks, mainly\n",
    "because of its simplicity and efficiency.\n",
    "\n",
    "The ReLU function is mathematically represented as:\n",
    "\n",
    "$$\\mathrm{ReLU}(x) = \\max(0,\\, x)$$\n",
    "\n",
    "This means that if the input is positive, it returns the input itself, and if the input is negative\n",
    "or zero, it returns zero.\n",
    "\n",
    "The gradient of the ReLU function is quite simple.\n",
    "It's either 0 (for $x \\leq 0$) or 1 (for $x > 0$).\n",
    "This is given by:\n",
    "\n",
    "$$\n",
    "    \\mathrm{ReLU}'(x) = \n",
    "        \\begin{cases} \n",
    "            0 & \\text{if } x \\leq 0 \\\\\n",
    "            1 & \\text{if } x > 0 \n",
    "        \\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "557d38686f477399"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Computational Efficiency**: The ReLU function is simple and can be implemented easily without\n",
    "   requiring any complex operations like exponentials.\n",
    "   This makes it computationally efficient.\n",
    "2. **Sparsity**: ReLU activation leads to sparsity.\n",
    "   When the output is zero, it's said to be \"inactive\", and when many neurons are inactive in a\n",
    "   layer, the resulting representations are sparse.\n",
    "   Sparse representations seem to be more beneficial than dense ones in deep learning models.\n",
    "3. **Mitigating the Vanishing Gradient Problem**: Traditional activation functions like sigmoid or\n",
    "   tanh squish their input into a small range between 0 and 1 or -1 and 1 respectively.\n",
    "   For deep networks, this could lead to gradients that are too small for the network to learn\n",
    "   effectively.\n",
    "   ReLU helps mitigate this problem, allowing models to learn faster and require less data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6fd8c77040c5ac5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drawbacks\n",
    "\n",
    "1. **Dying ReLU Problem**: Since the gradient for negative values is zero, during training, some\n",
    "   neurons might never activate, effectively getting knocked off during the training and not\n",
    "   contributing to the model.\n",
    "   This is called the \"dying ReLU\" problem.\n",
    "2. **Not Zero-Centered**: Unlike the tanh function, ReLU outputs are not zero-centered."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b78a58886fc5f8e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "888e1bd41799f435"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Example 1: Computing the ReLU of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22cefbf7477d6999"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from activations import relu\n",
    "\n",
    "x = torch.Tensor([-1.5, 0, 0.5, 2])\n",
    "result = relu(x)\n",
    "print(result)  # Expected: tensor([0., 0., 0.5, 2.])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35c524f47503dcbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 2: Computing the gradient of ReLU for a tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818ceecd943b52f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([-1.5, 0, 0.5, 2])\n",
    "gradient_result = relu(x, gradient=True)\n",
    "print(gradient_result)  # Expected: tensor([0., 1., 1., 1.])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b2b1721adb55fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 3: Using ReLU on higher-dimensional tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31c4c77a82621b82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([[-1, 1], [0, -2]])\n",
    "result = relu(x)\n",
    "print(result)\n",
    "# Expected:\n",
    "# tensor([[0., 1.],\n",
    "#         [0., 0.]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c00e6b43af67c9c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 4: Testing against PyTorch's built-in ReLU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aceee45a462a1637"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    actual = relu(x)\n",
    "    expected = torch.relu(x)\n",
    "    assert torch.allclose(actual, expected), f\"Expected {expected}, got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61aa27c6f8eda880"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CELU Activation Function\n",
    "\n",
    "The `CELU` (Continuously Differentiable Exponential Linear Units) activation function emerges as an enhancement over traditional ReLU and ELU activation functions. Its purpose is twofold:\n",
    "\n",
    "1. **Overcoming the Dying ReLU Problem**: By permitting negative values for inputs below zero, CELU mitigates the issue where neurons can sometimes become inactive and no longer update their weightsâ€”a phenomenon known as the \"dying ReLU\" problem.\n",
    "\n",
    "2. **Maintaining Smooth Gradients**: The function is designed to offer continuous differentiability, ensuring smooth gradients that aid in the optimization process.\n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "For an input \\( x \\) and a parameter \\( \\alpha > 0 \\), CELU is mathematically represented as:\n",
    "\n",
    "$$\n",
    "    \\mathrm{celu}(x, \\alpha) = \n",
    "    \\begin{cases}\n",
    "            x & \\text{if } x \\geq 0 \\\\\n",
    "            \\alpha (\\exp(\\frac{x}{\\alpha}) - 1) & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( x \\) denotes the input.\n",
    "- \\( \\alpha \\) is a tunable parameter governing the saturation rate for negative inputs, influencing how steeply the function saturates for values below zero.\n",
    "\n",
    "The gradient of the CELU function with respect to its input \\( x \\) is:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial x} = \n",
    "    \\begin{cases}\n",
    "            1 & \\text{if } x \\geq 0 \\\\\n",
    "            \\frac{\\text{celu}(x, \\alpha) - x e^{\\frac{x}{\\alpha}}}{\\alpha} & \\text{if } x < 0\n",
    "    \\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7a00890185b8935"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Advantages:\n",
    "\n",
    "1. **Avoiding the Dying ReLU Problem**: Unlike ReLU, which can \"kill\" neurons leading them to\n",
    "   output only zeros (especially during the training phase), CELU allows negative values for\n",
    "   inputs below zero.\n",
    "2. **Smooth Gradient**: Ensures smoother gradients compared to the original ReLU, which can help\n",
    "   improve optimization and convergence during training.\n",
    "3. **Configurable Saturation Rate**: The $\\alpha$ parameter allows for configuring how fast the \n",
    "   activation saturates for negative inputs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5761eb3e46a31b5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Disadvantages:\n",
    "\n",
    "1. **Computational Overhead**: Due to the exponential function, CELU can be more computationally expensive than simpler activation functions like ReLU.\n",
    "2. **Parameter Tuning**: Introducing the $\\alpha$ parameter can sometimes require additional tuning to get optimal performance, adding to the complexity of the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b52488a163bde9cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage:\n",
    "\n",
    "While CELU can be used in a variety of deep learning architectures, it's especially beneficial in scenarios where you observe the dying ReLU problem or when you want a smoother gradient for better optimization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a559d3362261e7bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "214bb221a9e5079e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 1: Computing the CELU of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1b8e8bb29552c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from activations import celu\n",
    "\n",
    "print(celu(torch.tensor([-1, 0, 1])))  # Output: tensor([-0.6321,  0.0000,  1.0000])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3980fe3c015e6de7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 2: Varying the Alpha Parameter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf4579e972a26b12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_with_alpha = celu(torch.tensor([-1, 0, 1]), alpha=0.5)\n",
    "print(result_with_alpha)  # Output: tensor([-0.4323,  0.0000,  1.0000])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7837b3a9d2b430fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 3: Computing the Gradient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0be1ec89ce227b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([-1, 0, 1])\n",
    "gradient_result = celu(x, gradient=True)\n",
    "print(gradient_result)  # Output: tensor([0.2642, 1.0000, 1.0000])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6900928d4bd9bd20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 4: Higher-dimensional Tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "555a264c27a5bc65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([[1, -1], [0, 2]])\n",
    "result = celu(x)\n",
    "print(result)\n",
    "# Output: \n",
    "# tensor([[ 1.0000, -0.6321],\n",
    "#         [ 0.0000,  2.0000]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "855016034a91b100"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 5: Testing against PyTorch's Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92dae1a987a90fa4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    x = torch.randn((10, 10))\n",
    "    actual = celu(x)\n",
    "    expected = torch.celu(x)\n",
    "    assert torch.allclose(actual, expected, atol=1e-4), f\"Expected {expected}, but got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa3c60fd4bdcdabb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swish Activation Function\n",
    "\n",
    "The `Swish` activation function, introduced by researchers at Google, is a smooth, non-monotonic function that has gained traction due to its superior performance in deep networks, especially when compared to the traditional ReLU function. Swish's self-gated property helps it provide more dynamic adaptability across various tasks, making it particularly effective in deeper architectures.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "The Swish function is given by the formula:\n",
    "\n",
    "$$\n",
    "\\mathrm{swish}(x) = x \\times \\mathrm{sigmoid}(\\beta x)\n",
    "$$\n",
    "\n",
    "### Properties:\n",
    "\n",
    "1. **Smoothness**: Swish is continuously differentiable, which ensures smooth gradients and assists in the optimization process.\n",
    "2. **Non-monotonicity**: Unlike ReLU and its variants, Swish is non-monotonic, introducing a form of regulation and adaptability in the network.\n",
    "3. **Self-Gated**: The function's adaptability arises from its self-gated nature, allowing each neuron to regulate its own activation based on its input.\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- **Superior Performance in Deep Networks**: Empirical results have demonstrated that Swish often outperforms other activation functions, especially in deeper networks.\n",
    "- **Computational Efficiency**: Despite being slightly more complex than ReLU, Swish retains a high level of computational efficiency.\n",
    "  \n",
    "### Gradient:\n",
    "\n",
    "The gradient of the Swish function with respect to its input $x$ is given by:\n",
    "\n",
    "$$\n",
    "    \\mathrm{swish}'(x) =\n",
    "            \\mathrm{sigmoid}(\\beta x)\n",
    "            + \\beta x \\times \\mathrm{sigmoid}(\\beta x)\n",
    "                \\times (1 - \\mathrm{sigmoid}(\\beta x))\n",
    "$$\n",
    "\n",
    "This gradient ensures the backpropagation process is smooth and efficient.\n",
    "\n",
    "In essence, the Swish activation function offers a blend of linearity and non-linearity, making it a compelling choice for many deep learning tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "284159e04f37fd81"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples\n",
    "\n",
    "#### 1. Basic Computation of Swish Function:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd98179eea349fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from swish import swish\n",
    "\n",
    "x = torch.Tensor([-1, 0, 1])\n",
    "output = swish(x)\n",
    "print(output)  # Expected output: tensor([-0.2689,  0.0000,  0.7311])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71a44a53bb3a50ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Computing Gradient of Swish Function:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abfb0e57e6fc12b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([-1, 0, 1])\n",
    "gradient = swish(x, gradient=True)\n",
    "print(gradient)  # Expected output: tensor([0.0723, 0.5000, 0.9277])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cd61bc6b876d0cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Using a Different Beta Value:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15c05e193a41aee7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([-1, 0, 1])\n",
    "output_with_beta = swish(x, beta=1.5)\n",
    "print(output_with_beta)  # Expected output: tensor([-0.1824,  0.0000,  0.8176])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "184dcd40ad40d4f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Handling Higher-Dimensional Tensors:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfd11412602767d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.Tensor([[0, 1], [-1, 2]])\n",
    "result = swish(x)\n",
    "print(result)\n",
    "# Expected output:\n",
    "# tensor([[0.0000, 0.7311],\n",
    "#         [-0.2689, 1.7616]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afbe5e49022d8c98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Benchmarking Against PyTorch's Implementation:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5787c10b4830170"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    actual, expected = swish(x), x * torch.sigmoid(x)  # Using PyTorch's built-in sigmoid for verification\n",
    "    assert torch.allclose(actual, expected, atol=1e-6), f\"Expected {expected}, but got {actual}\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae8f835684dafbe5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Activation Function\n",
    "\n",
    "The softmax function is a crucial activation function predominantly used in the output layers of classification neural networks. It transforms a vector of arbitrary real values into a probability distribution over multiple classes.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Given an input vector $\\mathbf{x} = [x_1, x_2, ..., x_k]$, the softmax function $\\mathrm{softmax}(\\mathbf{x})$ for a particular component $z_i$ is defined as:\n",
    "\n",
    "$$\\mathrm{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "1. **Output Range**: Each component of the output vector lies in the range (0, 1), making it interpretable as a probability.\n",
    "2. **Normalization**: The sum of all the components of the output vector is 1, ensuring it's a valid probability distribution.\n",
    "3. **Monotonicity**: If one component of the input vector increases while others remain constant, the corresponding component of the softmax output will also increase.\n",
    "4. **Sensitivity**: It amplifies the differences between the largest component and other components in the input vector.\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **Multiclass Classification**: Softmax is extensively used in neural networks for multiclass classification tasks. When the network needs to decide among multiple classes, the softmax function is typically used in the final layer, coupled with the categorical cross-entropy loss during training.\n",
    "2. **Reinforcement Learning**: In policy gradient methods, the softmax function helps in producing a probability distribution over actions.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "1. **Numerical Stability**: Direct computation can lead to numerical instability due to the exponentiation of large numbers. This can be mitigated by subtracting the maximum value in the input vector from all components of the vector before applying the softmax.\n",
    "2. **Choice of Loss Function**: When using softmax in neural networks, it's crucial to pair it with an appropriate loss function. The categorical cross-entropy loss is the most common choice.\n",
    "\n",
    "### Differences from Other Activation Functions\n",
    "\n",
    "Unlike sigmoid or tanh which operate element-wise and squish their input into a bounded range, softmax operates on vectors and ensures their output sums to 1. This makes it suitable for producing probability distributions over multiple categories."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a2a3feca400a66d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cc9c1e871a432a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Basic 1D Vector"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edadb2d184a65400"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from activations import softmax\n",
    "\n",
    "softmax_values = softmax(torch.tensor([2.0, 1.0, 0.1]), dim=0)\n",
    "print(softmax_values)\n",
    "# Output: tensor([0.6590, 0.2424, 0.0986])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9960f9730016da6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 2D Tensor (Matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c292d339a0135bd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a 2D tensor\n",
    "tensor_2d = torch.Tensor([[1.0, 2.0, 3.0], [0.1, -0.5, 0.2]])\n",
    "\n",
    "# Compute softmax values along dim=1\n",
    "softmax_matrix = softmax(tensor_2d, dim=1)\n",
    "\n",
    "print(softmax_matrix)\n",
    "# Expected output:\n",
    "# tensor([[0.0900, 0.2447, 0.6652],\n",
    "#         [0.3768, 0.2068, 0.4164]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "425862856fb03b53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Using the Stable Softmax"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5d3b006b26fd801"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensor = torch.Tensor([50.0, 60.0, 70.0])\n",
    "\n",
    "# Without stability\n",
    "values_unstable = softmax(tensor, dim=0, stable=False)\n",
    "\n",
    "# With stability\n",
    "values_stable = softmax(tensor, dim=0, stable=True)\n",
    "\n",
    "print(values_unstable)  # Might produce unexpected results due to numerical issues\n",
    "\n",
    "print(values_stable)    # Should produce valid probabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db74a6d86e475adb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. High-dimensional Tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "605db54c888f9fe4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensor_3d = torch.rand(2, 3, 4)  # 3D tensor\n",
    "\n",
    "# Apply softmax along the second dimension\n",
    "softmax_3d = softmax(tensor_3d, dim=1)\n",
    "print(softmax_3d)\n",
    "\n",
    "# This will convert each 3x4 matrix slice of the tensor into a probability distribution along its rows."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f4ca686c28ee315"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Testing Against PyTorch's Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "150d7b371ab841c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    actual = softmax(x, dim=1)\n",
    "    expected = torch.softmax(x, dim=1)\n",
    "    assert torch.allclose(actual, expected, atol=1e-4), f\"Expected {expected}, but got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7c3b1d22ab21f41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
