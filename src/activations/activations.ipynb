{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc256b56b6985559",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e5cb4dd4620ed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "The `activations` module offers a collection of popular activation functions \n",
    "essential for neural network designs.\n",
    "Along with the primary function definitions, this module calculates the \n",
    "gradients for each, aiding in understanding and applying the back-propagation \n",
    "algorithm.\n",
    "Notably, the `softmax` function is an exception due to its inherent multi-input,\n",
    "multi-output structure, necessitating a unique gradient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path('..') / '..'\n",
    "!pip install -q -r {ROOT_DIR / 'requirements.txt'}\n",
    "\n",
    "import torch  # needed for running the examples\n",
    "from tqdm import tqdm  # prettier progress bars\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:31.971517600Z",
     "start_time": "2023-08-19T03:43:29.913442600Z"
    }
   },
   "id": "3f731ab65d24aa4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's an improved and restructured version:\n",
    "\n",
    "## Sigmoid Activation Function\n",
    "\n",
    "The sigmoid function is a type of activation function that is primarily used in binary \n",
    "classification tasks.\n",
    "It maps any input to a value between 0 and 1, which can often be used to represent the \n",
    "probability that a given input point belongs to the positive class.\n",
    "\n",
    "Mathematically, the sigmoid function is given by:\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Its derivative, crucial for the backpropagation algorithm, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}'(x) = \\mathrm{sigmoid}(x)(1 - \\mathrm{sigmoid}(x))\n",
    "$$\n",
    "\n",
    "However, it's worth noting that the sigmoid function can lead to vanishing gradients when its \n",
    "input is very high or very low."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7593e414e6ae192e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1367b490e8bc038"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Computing the sigmoid of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b7a374152bd8910"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 0.7311, 0.8808])\n"
     ]
    }
   ],
   "source": [
    "from activations import sigmoid\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "result = sigmoid(x)\n",
    "print(result)  # Outputs: tensor([0.5000, 0.7311, 0.8808])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:31.978882100Z",
     "start_time": "2023-08-19T03:43:31.971517600Z"
    }
   },
   "id": "de0027d14499c660"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Determining the gradient of the sigmoid for a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf9c3112fd10d9e1"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.1966, 0.1050])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([0, 1, 2])\n",
    "gradient_result = sigmoid(x, gradient=True)\n",
    "print(gradient_result)  # Outputs: tensor([0.2500, 0.1966, 0.1050])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:31.993511Z",
     "start_time": "2023-08-19T03:43:31.976887Z"
    }
   },
   "id": "e4a4643ecc84fb99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Handling higher-dimensional tensors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8edd58eda6bb0a2"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.7311],\n",
      "        [0.2689, 0.8808]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[0, 1], [-1, 2]])\n",
    "result = sigmoid(x)\n",
    "print(result)\n",
    "# Outputs: \n",
    "# tensor([[0.5000, 0.7311],\n",
    "#         [0.2689, 0.8808]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:32.046042700Z",
     "start_time": "2023-08-19T03:43:31.990509500Z"
    }
   },
   "id": "71e01266dd43e7c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Verifying against PyTorch's built-in implementation:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9d565802896584e"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 42.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    our_implementation = sigmoid(x)\n",
    "    pytorch_implementation = torch.sigmoid(x)\n",
    "    assert torch.allclose(our_implementation, pytorch_implementation), \\\n",
    "        f\"Expected {pytorch_implementation}, but got {our_implementation}\"\n",
    "print(\"All tests passed!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:34.373280Z",
     "start_time": "2023-08-19T03:43:32.013044300Z"
    }
   },
   "id": "915387526313a49f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tanh"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63d5430cac08ac3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tanh Activation Function\n",
    "\n",
    "The hyperbolic tangent, or simply $\\text{tanh}$, is another prevalent activation function used\n",
    "in neural networks.\n",
    "Its outputs range between -1 and 1, making it zero-centered, which can help mitigate some of\n",
    "the issues observed with non-zero-centered activation functions like the sigmoid.\n",
    "\n",
    "Mathematically, the $\\text{tanh}$ function is expressed as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Or, equivalently, as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}(x) = 2 \\times \\mathrm{sigmoid}(2x) - 1\n",
    "$$\n",
    "\n",
    "The derivative of $\\text{tanh}$, useful for backpropagation, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tanh}'(x) = 1 - \\mathrm{tanh}^2(x)\n",
    "$$\n",
    "\n",
    "Compared to the sigmoid function, $\\text{tanh}$ tends to be preferred for hidden layers due to\n",
    "its zero-centered nature.\n",
    "Still, it shares the vanishing gradient problem for extremely high or low inputs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a985394154c4e45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Examples\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d4e8e0c651cc57c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 1. Computing the $\\text{tanh}$ of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77cb37f4eed9ab57"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.7616, 0.9640])\n"
     ]
    }
   ],
   "source": [
    "from activations import tanh\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "result = tanh(x)\n",
    "print(result)  # Expected: tensor([0.0000, 0.7616, 0.9640])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:34.382283Z",
     "start_time": "2023-08-19T03:43:34.374280100Z"
    }
   },
   "id": "59ba8fb0703a31f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Determining the gradient of $\\text{tanh}$ for a tensor:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ea58baf1ae8f59b"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.4200, 0.0707])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([0, 1, 2])\n",
    "gradient_result = tanh(x, gradient=True)\n",
    "print(gradient_result)  # Expected: tensor([1.0000, 0.4200, 0.0707])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:34.456314200Z",
     "start_time": "2023-08-19T03:43:34.380282300Z"
    }
   },
   "id": "59f9e742846130ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Handling higher-dimensional tensors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d2a3c3964cf3ee"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.7616],\n",
      "        [-0.7616,  0.9640]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[0, 1], [-1, 2]])\n",
    "result = tanh(x)\n",
    "print(result)\n",
    "# Expected: \n",
    "# tensor([[ 0.0000,  0.7616],\n",
    "#         [-0.7616,  0.9640]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:34.460850600Z",
     "start_time": "2023-08-19T03:43:34.397309500Z"
    }
   },
   "id": "dd3937e5ed106e05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Verifying against PyTorch's built-in implementation:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "273a3801e977fa56"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    actual, expected = tanh(x), torch.tanh(x)\n",
    "    assert torch.allclose(actual, expected, atol=1e-7), f\"Expected {expected}, but got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:36.884849900Z",
     "start_time": "2023-08-19T03:43:34.416309Z"
    }
   },
   "id": "8f088d43c476d891"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU (Rectified Linear Unit)\n",
    "\n",
    "ReLU, or Rectified Linear Unit, is one of the most widely used activation functions in deep learning\n",
    "models.\n",
    "It is especially popular in convolutional neural networks and deep feed-forward networks, mainly\n",
    "because of its simplicity and efficiency.\n",
    "\n",
    "The ReLU function is mathematically represented as:\n",
    "\n",
    "$$\\mathrm{ReLU}(x) = \\max(0,\\, x)$$\n",
    "\n",
    "This means that if the input is positive, it returns the input itself, and if the input is negative\n",
    "or zero, it returns zero.\n",
    "\n",
    "The gradient of the ReLU function is quite simple.\n",
    "It's either 0 (for $x \\leq 0$) or 1 (for $x > 0$).\n",
    "This is given by:\n",
    "\n",
    "$$\n",
    "    \\mathrm{ReLU}'(x) = \n",
    "        \\begin{cases} \n",
    "            0 & \\text{if } x \\leq 0 \\\\\n",
    "            1 & \\text{if } x > 0 \n",
    "        \\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "557d38686f477399"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Computational Efficiency**: The ReLU function is simple and can be implemented easily without\n",
    "   requiring any complex operations like exponentials.\n",
    "   This makes it computationally efficient.\n",
    "2. **Sparsity**: ReLU activation leads to sparsity.\n",
    "   When the output is zero, it's said to be \"inactive\", and when many neurons are inactive in a\n",
    "   layer, the resulting representations are sparse.\n",
    "   Sparse representations seem to be more beneficial than dense ones in deep learning models.\n",
    "3. **Mitigating the Vanishing Gradient Problem**: Traditional activation functions like sigmoid or\n",
    "   tanh squish their input into a small range between 0 and 1 or -1 and 1 respectively.\n",
    "   For deep networks, this could lead to gradients that are too small for the network to learn\n",
    "   effectively.\n",
    "   ReLU helps mitigate this problem, allowing models to learn faster and require less data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6fd8c77040c5ac5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drawbacks\n",
    "\n",
    "1. **Dying ReLU Problem**: Since the gradient for negative values is zero, during training, some\n",
    "   neurons might never activate, effectively getting knocked off during the training and not\n",
    "   contributing to the model.\n",
    "   This is called the \"dying ReLU\" problem.\n",
    "2. **Not Zero-Centered**: Unlike the tanh function, ReLU outputs are not zero-centered."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b78a58886fc5f8e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "888e1bd41799f435"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Example 1: Computing the ReLU of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22cefbf7477d6999"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.5000, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "from activations import relu\n",
    "\n",
    "x = torch.Tensor([-1.5, 0, 0.5, 2])\n",
    "result = relu(x)\n",
    "print(result)  # Expected: tensor([0., 0., 0.5, 2.])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:36.886849700Z",
     "start_time": "2023-08-19T03:43:36.881099400Z"
    }
   },
   "id": "35c524f47503dcbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 2: Computing the gradient of ReLU for a tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818ceecd943b52f9"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([-1.5, 0, 0.5, 2])\n",
    "gradient_result = relu(x, gradient=True)\n",
    "print(gradient_result)  # Expected: tensor([0., 1., 1., 1.])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:36.902071600Z",
     "start_time": "2023-08-19T03:43:36.885850100Z"
    }
   },
   "id": "49b2b1721adb55fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 3: Using ReLU on higher-dimensional tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31c4c77a82621b82"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[-1, 1], [0, -2]])\n",
    "result = relu(x)\n",
    "print(result)\n",
    "# Expected:\n",
    "# tensor([[0., 1.],\n",
    "#         [0., 0.]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:36.943106200Z",
     "start_time": "2023-08-19T03:43:36.898087700Z"
    }
   },
   "id": "c00e6b43af67c9c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 4: Testing against PyTorch's built-in ReLU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aceee45a462a1637"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    x = torch.randn((100, 100, 100))\n",
    "    actual = relu(x)\n",
    "    expected = torch.relu(x)\n",
    "    assert torch.allclose(actual, expected), f\"Expected {expected}, got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:38.907674100Z",
     "start_time": "2023-08-19T03:43:36.913072100Z"
    }
   },
   "id": "61aa27c6f8eda880"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CELU Activation Function\n",
    "\n",
    "The `CELU` (Continuously Differentiable Exponential Linear Units) activation function is a \n",
    "modified version of the traditional ReLU and ELU activation functions.\n",
    "It aims to resolve the dying ReLU problem by enabling negative values for input below zero,\n",
    "while still preserving smooth gradients for optimization.\n",
    "\n",
    "Formally:\n",
    "Given an input $x$ and a parameter $\\alpha > 0$, the CELU function is defined as:\n",
    "\n",
    "$$\n",
    "    \\mathrm{celu}(x, \\alpha) = \\begin{cases}\n",
    "            x                                   & \\text{if } x \\geq 0 \\\\\n",
    "            \\alpha (\\exp(\\frac{x}{\\alpha}) - 1) & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input.\n",
    "- $\\alpha$ is a parameter that determines the saturation rate for negative inputs.\n",
    "\n",
    "The gradient of the CELU function w.r.t. its input $x$ is:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial x} = \\begin{cases}\n",
    "            1 & \\text{if } x \\geq 0 \\\\\n",
    "            \\frac{\\text{celu}(x, \\alpha) - x e^{\\frac{x}{\\alpha}}}{\\alpha}\n",
    "                & \\text{if } x < 0\n",
    "        \\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7a00890185b8935"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Advantages:\n",
    "\n",
    "1. **Avoiding the Dying ReLU Problem**: Unlike ReLU, which can \"kill\" neurons leading them to\n",
    "   output only zeros (especially during the training phase), CELU allows negative values for\n",
    "   inputs below zero.\n",
    "2. **Smooth Gradient**: Ensures smoother gradients compared to the original ReLU, which can help\n",
    "   improve optimization and convergence during training.\n",
    "3. **Configurable Saturation Rate**: The $\\alpha$ parameter allows for configuring how fast the \n",
    "   activation saturates for negative inputs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5761eb3e46a31b5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Disadvantages:\n",
    "\n",
    "1. **Computational Overhead**: Due to the exponential function, CELU can be more computationally expensive than simpler activation functions like ReLU.\n",
    "2. **Parameter Tuning**: Introducing the $\\alpha$ parameter can sometimes require additional tuning to get optimal performance, adding to the complexity of the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b52488a163bde9cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage:\n",
    "\n",
    "While CELU can be used in a variety of deep learning architectures, it's especially beneficial in scenarios where you observe the dying ReLU problem or when you want a smoother gradient for better optimization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a559d3362261e7bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "214bb221a9e5079e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example 1: Computing the CELU of a tensor:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1b8e8bb29552c3"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6321,  0.0000,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "from activations import celu\n",
    "\n",
    "print(celu(torch.tensor([-1, 0, 1])))  # Output: tensor([-0.6321,  0.0000,  1.0000])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:38.909672600Z",
     "start_time": "2023-08-19T03:43:38.903672200Z"
    }
   },
   "id": "3980fe3c015e6de7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 2: Varying the Alpha Parameter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf4579e972a26b12"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4323,  0.0000,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "result_with_alpha = celu(torch.tensor([-1, 0, 1]), alpha=0.5)\n",
    "print(result_with_alpha)  # Output: tensor([-0.4323,  0.0000,  1.0000])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:38.969109100Z",
     "start_time": "2023-08-19T03:43:38.908673400Z"
    }
   },
   "id": "7837b3a9d2b430fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 3: Computing the Gradient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0be1ec89ce227b5"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2642,  1.0000,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([-1, 0, 1])\n",
    "gradient_result = celu(x, gradient=True)\n",
    "print(gradient_result)  # Output: tensor([0.2642, 1.0000, 1.0000])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:38.970108900Z",
     "start_time": "2023-08-19T03:43:38.922673900Z"
    }
   },
   "id": "6900928d4bd9bd20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 4: Higher-dimensional Tensors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "555a264c27a5bc65"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -0.6321],\n",
      "        [ 0.0000,  2.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, -1], [0, 2]])\n",
    "result = celu(x)\n",
    "print(result)\n",
    "# Output: \n",
    "# tensor([[ 1.0000, -0.6321],\n",
    "#         [ 0.0000,  2.0000]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:38.971109200Z",
     "start_time": "2023-08-19T03:43:38.934458Z"
    }
   },
   "id": "855016034a91b100"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example 5: Testing against PyTorch's Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92dae1a987a90fa4"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    x = torch.randn((10, 10))\n",
    "    actual = celu(x)\n",
    "    expected = torch.celu(x)\n",
    "    assert torch.allclose(actual, expected, atol=1e-4), f\"Expected {expected}, but got {actual}\"\n",
    "print(\"All tests passed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-19T03:43:39.010389600Z",
     "start_time": "2023-08-19T03:43:38.953593Z"
    }
   },
   "id": "fa3c60fd4bdcdabb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
