{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path('..') / '..'\n",
    "!pip install -r {ROOT_DIR/ 'requirements.txt'}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "829cb43cfed9f79e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks, often referred to as artificial neural networks (ANNs), are computational systems that attempt to mimic the way the human brain processes information. Inspired by biological neural structures, ANNs are central to deep learning, a subset of machine learning. Over the past decade, they've driven many AI advancements, setting benchmark performance in tasks like image recognition, natural language processing, and game-playing strategies.\n",
    "\n",
    "## Structure\n",
    "\n",
    "A neural network is usually built with several layers of interconnected nodes or \"neurons\". Every connection has a weight that's adjusted during training. The primary layers of a neural network include:\n",
    "\n",
    "1. **Input Layer:** Accepts input features and sends them to the next layer.\n",
    "2. **Hidden Layers:** Intermediate layers that handle input data by processing them through weighted connections and activation functions.\n",
    "3. **Output Layer:** Yields the ultimate predictions or categorizations.\n",
    "\n",
    "Activation functions introduce non-linearity when applied by neurons to their inputs. This non-linearity is crucial as it enables the network to capture complex patterns and relationships in the data.\n",
    "\n",
    "In this notebook, we will employ a straightforward feed-forward neural network to categorize images from the MNIST dataset. This network boasts two hidden layers with 16 units each, using the ReLU activation function. The output layer consists of 10 units, each representing a class, and applies the softmax activation function. After training the network, its parameters were saved. We will now load them to visualize a few sample predictions.\n",
    "\n",
    "## Feed-Forward Network\n",
    "\n",
    "Feed-forward neural networks are a category of ANNs where the units don't form cyclical connections. This ensures the data flows only in a single directionâ€”starting from the input nodes, moving through any hidden nodes, and culminating at the output nodes. These networks lack the feedback loops that would reintroduce the model's outputs back into itself, differentiating feed-forward networks from recurrent neural networks. They focus on distinct data hierarchies, unlike convolutional networks that prioritize spatial hierarchies, or RNNs which are designed for sequences and possess a form of memory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41734d4901bd26ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MNIST Dataset Overview\n",
    "\n",
    "The MNIST dataset comprises 70,000 grayscale images of handwritten digits. Specifically:\n",
    "\n",
    "- **Image Dimensions**: Each image measures 28x28 pixels.\n",
    "- **Pixel Representation**: Pixels range from 0 (black) to 255 (white).\n",
    "- **Dataset Split**:\n",
    "  - **Training Set**: 60,000 images\n",
    "  - **Test Set**: 10,000 images\n",
    "\n",
    "Due to its simplicity and established benchmarks, MNIST is a popular choice for introductory machine learning exercises, often likened to the \"Hello, World!\" of machine learning.\n",
    "\n",
    "### Loading and Visualizing the MNIST Dataset\n",
    "\n",
    "Let's load the dataset and randomly visualize some of the handwritten digits:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7094a57a9a538f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.axes import Axes\n",
    "from numpy import ndarray\n",
    "from matplotlib import pyplot\n",
    "from torchvision.transforms import ToTensor\n",
    "from utils import DATA_PATH\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "def mnist_dataset() -> MNIST:\n",
    "    \"\"\"Load the MNIST dataset\"\"\"\n",
    "    return MNIST(str(DATA_PATH / \"mnist\"), train=False, transform=ToTensor(), download=True)\n",
    "\n",
    "\n",
    "# Define the number of examples to visualize\n",
    "N_EXAMPLES = 3\n",
    "\n",
    "\n",
    "def setup_figure() -> ndarray[Axes]:\n",
    "    \"\"\"Set up a figure for visualization\"\"\"\n",
    "    fig = pyplot.figure(figsize=(6, N_EXAMPLES * 3))\n",
    "    grid_spec = fig.add_gridspec(1, 3, hspace=2)\n",
    "    axs = grid_spec.subplots(sharey='row')\n",
    "    return axs\n",
    "\n",
    "\n",
    "def display_examples(*axs: pyplot.Axes):\n",
    "    \"\"\"Display a few examples from the MNIST dataset\"\"\"\n",
    "    for i in range(N_EXAMPLES):\n",
    "        idx = random.randint(0, len(mnist_dataset()))\n",
    "        img, label = mnist_dataset()[idx]\n",
    "        view = img.view(28, 28).numpy()\n",
    "        axs[i].set_title(f\"Class: {label}\")\n",
    "        axs[i].imshow(view)\n",
    "\n",
    "\n",
    "display_examples(*setup_figure())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d2a06cf609dd86f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using a Pre-Trained Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ba11dc54f72eb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading Model Parameters from Disk\n",
    "\n",
    "In a neural network, parameters constitute the weights and biases between units. They're essential because they are learned and adjusted during the training phase, allowing the model to make accurate predictions or classifications.\n",
    "\n",
    "For our task, we've previously trained a model on the MNIST dataset and saved its parameters to disk. Let's load these parameters and initialize our neural network model for further predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37673b2d5f821e24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load weights and biases for hidden layers\n",
    "weights = [torch.from_numpy(loadtxt(DATA_PATH / f\"W{i}.txt\")).float() for i in (1, 2)]\n",
    "biases = [torch.from_numpy(loadtxt(DATA_PATH / f\"b{i}.txt\")).float() for i in (1, 2)]\n",
    "\n",
    "# Load weights and biases for the output layer\n",
    "output_weights = torch.from_numpy(loadtxt(DATA_PATH / \"U.txt\")).float()\n",
    "output_biases = torch.from_numpy(loadtxt(DATA_PATH / \"c.txt\")).float()\n",
    "\n",
    "# Initialize the neural network model\n",
    "MNIST_MODEL = FeedForwardNetwork(\n",
    "    n_features=784,\n",
    "    hidden_layer_sizes=[16, 16],\n",
    "    activation_functions=[relu, relu],\n",
    "    n_classes=10,\n",
    ")\n",
    "\n",
    "# Inject the loaded parameters into the model\n",
    "MNIST_MODEL.load_parameters(weights, output_weights, biases, output_biases)\n",
    "\n",
    "# Display the initialized model\n",
    "print(MNIST_MODEL)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd77cf48854b6ef4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Making Predictions with the Model\n",
    "\n",
    "After successfully loading the model and dataset, it's time to use our model to make predictions. We'll select a subset of images from the MNIST dataset at random and visualize their true and predicted class labels."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2ab362a591ceb76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_predictions(*axs: pyplot.Axes) -> None:\n",
    "    \"\"\"\n",
    "    Visualize predictions for a subset of the MNIST dataset.\n",
    "\n",
    "    This function randomly selects images from the MNIST dataset, makes predictions using\n",
    "    the loaded model, and visualizes the images alongside their true and predicted class labels.\n",
    "\n",
    "    :param axs: A list of Axes objects to display the images and predictions.\n",
    "    \"\"\"\n",
    "    for i in range(N_EXAMPLES):\n",
    "        idx = random.randint(0, len(mnist_dataset()))\n",
    "        img, true_label = mnist_dataset()[idx]\n",
    "        img_view = img.view(28, 28).numpy()\n",
    "\n",
    "        # Predicting the class label using the model\n",
    "        pred_prob, pred_label = torch.max(MNIST_MODEL(img.view(1, 784)), dim=1)\n",
    "\n",
    "        # Displaying the image along with true and predicted labels\n",
    "        axs[i].imshow(img_view)\n",
    "        axs[i].set_title(\n",
    "            f\"True Class: {true_label}\\n\"\n",
    "            f\"Predicted Class: {pred_label.item()}\\n\"\n",
    "            f\"Confidence: {pred_prob.item():.2f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Using the function to display predictions\n",
    "display_predictions(*setup_figure())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a1bfc1269f5a8c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluating Model Performance\n",
    "\n",
    "We want to see how well our model does on all the pictures we have. We'll use a function that goes through all the images in small groups, makes guesses with our model, and then counts how many guesses are right. In the end, we'll see what percentage of the guesses were correct."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "946ded82ce4c3a3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision.datasets import VisionDataset\n",
    "from networks import NeuralNetwork\n",
    "\n",
    "\n",
    "def evaluate_network(\n",
    "        network: NeuralNetwork,\n",
    "        dataset: VisionDataset,\n",
    "        batch_size: int = 100,\n",
    "        device: Device = Device.CPU,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a neural network on a given vision dataset.\n",
    "\n",
    "    This function iterates over the dataset using batches, computes predictions for each\n",
    "    batch using the provided network, and tracks the number of correct predictions.\n",
    "    At the end, it prints the accuracy of the network on the dataset.\n",
    "\n",
    "    :param network: The neural network model to be evaluated.\n",
    "    :param dataset: The dataset on which the network is evaluated.\n",
    "    :param batch_size: The size of the batches in which the dataset is divided for\n",
    "                       evaluation.\n",
    "                       Default is 100.\n",
    "    :param device: The device on which the computations are performed (CPU or GPU).\n",
    "\n",
    "    __Note:__\n",
    "\n",
    "    - This function assumes that the network's forward method outputs raw scores (logits)\n",
    "      for each class.\n",
    "    - The accuracy is computed as the percentage of correct predictions over the total\n",
    "      number of samples in the dataset.\n",
    "    \"\"\"\n",
    "    network.to(device)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    n_correct = 0\n",
    "    for x, y in tqdm(data_loader):\n",
    "        view: torch.Tensor = x.view(-1, network.input_size).to(device)\n",
    "        predictions: torch.Tensor = torch.max(network(view), dim=1)[1]\n",
    "        n_correct += torch.sum(torch.eq(predictions, y.to(device))).item()\n",
    "\n",
    "    print(f\"Accuracy: {(n_correct / len(dataset) * 100):.2f}%\")\n",
    "\n",
    "\n",
    "evaluate_network(MNIST_MODEL, mnist_dataset(), device=Device.CPU)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b967ec6c673db514"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training a Neural Network with Backpropagation\n",
    "\n",
    "Backpropagation is like a teacher for neural networks. It helps the network learn from mistakes by making small changes to its internal settings. Here's how it works:\n",
    "\n",
    "1. **Learning from Examples:** We show the network many examples and tell it the correct answers.\n",
    "2. **Making Guesses:** The network tries to guess the answer for each example.\n",
    "3. **Checking Mistakes:** After guessing, we check how far off its guess was from the correct answer.\n",
    "4. **Learning from Mistakes:** Using the mistakes it made, the network fine-tunes its internal settings to guess better next time.\n",
    "5. **Repeat:** We keep showing examples until the network gets good at guessing right.\n",
    "\n",
    "The magic of backpropagation is in step 4, where it figures out which settings to tweak and by how much. This \"tweaking\" is done using a math trick called gradient descent."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d1b614c4ff7149c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Checking if Gradients are Correct: Gradient Checking\n",
    "\n",
    "Imagine you've got a math formula, and you've made some changes to it. You'd want to double-check if your changes were right. That's what gradient checking does for neural networks.\n",
    "\n",
    "In simple terms, gradient checking compares two methods of finding gradients (slopes). One method uses the standard backpropagation technique. The other uses a quick-and-dirty method called \"finite difference approximation.\" If both methods give similar answers, we can be pretty sure our backpropagation is set up correctly.\n",
    "\n",
    "Here's a simple way to do gradient checking:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a079efc81c08a434"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_gradients(epsilon: float = 1e-6):\n",
    "    # Disable tracking computations\n",
    "    with torch.no_grad():\n",
    "        # Set some basics and random data\n",
    "        samples = 100\n",
    "        input_size = 300\n",
    "        output_classes = 10\n",
    "        # Set up a basic neural network\n",
    "        network = FeedForwardNetwork(input_size, [100, 200], [sigmoid, relu], output_classes)\n",
    "        parameters = list(network.parameters())\n",
    "        # Random input\n",
    "        input_data = torch.randn(samples, input_size)\n",
    "        # Make random target labels\n",
    "        labels = torch.zeros(samples, output_classes)\n",
    "        targets = torch.randint(0, output_classes, (samples,))\n",
    "        labels[torch.arange(samples), targets] = 1\n",
    "        for param in parameters:\n",
    "            # Check the loss when we reduce the parameter a tiny bit\n",
    "            param -= epsilon\n",
    "            pred_minus = network(input_data)\n",
    "            loss_minus = cross_entropy(pred_minus, labels)\n",
    "            # Check the loss when we increase the parameter a tiny bit\n",
    "            param += 2 * epsilon\n",
    "            pred_plus = network(input_data)\n",
    "            loss_plus = cross_entropy(pred_plus, labels)\n",
    "            # Quick-and-dirty gradient calculation\n",
    "            estimated_gradient = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            # Bring parameter back to original\n",
    "            param -= epsilon\n",
    "            # Get the actual gradient using backpropagation\n",
    "            pred = network(input_data)\n",
    "            network.backward(input_data, labels, pred)\n",
    "            # See how different the two gradients are\n",
    "            difference = torch.abs(estimated_gradient - torch.mean(param.grad))\n",
    "            print(f\"Difference between estimated and real gradient: {difference}\")\n",
    "\n",
    "\n",
    "# Run our gradient check\n",
    "check_gradients()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23784c7957683169"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training the Model\n",
    "\n",
    "Now that we've verified that our gradients are correct, we can train our model. We'll use a RandomDataset. This dataset generates random data and labels on the fly. It's useful for testing and debugging."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110bea0705b2945b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(losses: list[float], accuracies: list[float]):\n",
    "    \"\"\"Plot the loss and accuracy of the model during training\"\"\"\n",
    "    fig_loss = pyplot.figure(1)\n",
    "    loss_ax = fig_loss.add_subplot(111)\n",
    "    loss_ax.set_title(\"Loss\")\n",
    "    loss_ax.set_xlabel(\"epochs\")\n",
    "    loss_ax.set_ylabel(\"loss\")\n",
    "    loss_ax.plot(losses, c=\"r\")\n",
    "\n",
    "    fig_accuracy = pyplot.figure(2)\n",
    "    accuracy_ax = fig_accuracy.add_subplot(111)\n",
    "    accuracy_ax.set_title(\"Accuracy\")\n",
    "    accuracy_ax.set_xlabel(\"epochs\")\n",
    "    accuracy_ax.set_ylabel(\"acc\")\n",
    "    accuracy_ax.plot(accuracies, c=\"b\")\n",
    "    pyplot.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df04e4e0e7706d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import SizedDataset\n",
    "\n",
    "\n",
    "def convert_to_one_hot(tensor: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert tensor to one-hot encoding based on provided labels.\n",
    "    \"\"\"\n",
    "    one_hot = torch.zeros_like(tensor)\n",
    "    one_hot[torch.arange(tensor.size(0)), labels] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def train_one_batch(network: FeedForwardNetwork, optimizer: StochasticGradientDescent, x: torch.Tensor,\n",
    "                    y: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Train the network on a single batch of data.\n",
    "    \"\"\"\n",
    "    y_pred = network(x)\n",
    "    y_onehot = convert_to_one_hot(y_pred, y)\n",
    "    network.backward(x, y_onehot, y_pred)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def report_progress(epoch: int, accuracy: float, loss: float, avg_time: float):\n",
    "    \"\"\"\n",
    "    Report training progress.\n",
    "    \"\"\"\n",
    "    print(f\"\\rEpoch:{epoch:03d} Accuracy:{accuracy:.2f}% Loss:{loss:.4f} Time/epoch:{avg_time:.3f}s\", end='')\n",
    "\n",
    "\n",
    "def calculate_average_time(previous_avg: float, current_time: float, epoch: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average time taken per epoch.\n",
    "    \"\"\"\n",
    "    return (previous_avg * (epoch - 1) + current_time) / epoch\n",
    "\n",
    "\n",
    "def train_feed_forward_network(\n",
    "        network: FeedForwardNetwork,\n",
    "        dataset: SizedDataset | VisionDataset,\n",
    "        optimizer: StochasticGradientDescent,\n",
    "        epochs: int = 1,\n",
    "        batch_size: int = 1,\n",
    "        reports_every: int = 1,\n",
    "        device=Device.CPU\n",
    ") -> tuple[list[float], list[float]]:\n",
    "    network.to(device)\n",
    "    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    dataset_size = len(dataset)\n",
    "    average_time_per_epoch = 0\n",
    "    losses, accuracies = [], []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = timer()\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.view(x.size(0), -1).float().to(device), y.to(device)\n",
    "            train_one_batch(network, optimizer, x, y)\n",
    "        average_time_per_epoch = calculate_average_time(average_time_per_epoch, timer() - epoch_start_time, epoch)\n",
    "        if epoch % reports_every == 0:\n",
    "            x_all = dataset.data.view(dataset_size, -1).float().to(device)\n",
    "            true_labels = dataset.targets.to(device)\n",
    "            predicted_output = network(x_all).to(device)\n",
    "            onehot_prediction = convert_to_one_hot(predicted_output, true_labels)\n",
    "            loss = cross_entropy(predicted_output, onehot_prediction)\n",
    "            losses.append(loss)\n",
    "            predicted_labels = torch.argmax(predicted_output, dim=1)\n",
    "            accuracy = 100 * (predicted_labels == true_labels).sum().item() / dataset_size\n",
    "            accuracies.append(accuracy)\n",
    "            report_progress(epoch, accuracy, loss, average_time_per_epoch)\n",
    "    return losses, accuracies\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a7fe3dc68f48367"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_network_on_dataset(dataset_class):\n",
    "    # Hyperparameters\n",
    "    n_samples = 2000\n",
    "    n_features = 300\n",
    "    n_classes = 10\n",
    "    hidden_layer_sizes = [300, 400]\n",
    "    activation_functions = [celu, relu]\n",
    "    activation_function_parameters = [float(n_classes), None]\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "\n",
    "    # Initialize network\n",
    "    network = FeedForwardNetwork(n_features, hidden_layer_sizes, activation_functions,\n",
    "                                 n_classes, activation_function_parameters)\n",
    "\n",
    "    # Generate dataset based on the provided dataset class\n",
    "    dataset = dataset_class(n_samples, n_features, n_classes)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = StochasticGradientDescent(network.parameters(), learning_rate=learning_rate)\n",
    "\n",
    "    # Train network\n",
    "    with torch.no_grad():\n",
    "        losses, accuracies = train_feed_forward_network(network, dataset, optimizer,\n",
    "                                                        epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Plot results\n",
    "    plot_loss_and_accuracy(losses, accuracies)\n",
    "\n",
    "\n",
    "train_network_on_dataset(RandomUniformDataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1825dedf74b9dcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import BernoulliDataset\n",
    "\n",
    "train_network_on_dataset(BernoulliDataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f1e23843cb3011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import RandomNormalDataset\n",
    "\n",
    "train_network_on_dataset(RandomNormalDataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "570766839a445d6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training the Model on MNIST\n",
    "\n",
    "Having introduced the MNIST dataset and our chosen training methodology, let's delve into the specifics of putting them into action."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41ce62e0bfe8d1cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_network_on_mnist_dataset():\n",
    "    # Hyperparameters\n",
    "    n_features = 784\n",
    "    n_classes = 10\n",
    "    hidden_layer_sizes = [512, 1024, 128]\n",
    "    activation_functions = [relu, relu, relu]\n",
    "    learning_rate = 1e-5\n",
    "    epochs = 30\n",
    "    batch_size = 32\n",
    "    # Initialize network\n",
    "    network = FeedForwardNetwork(n_features, hidden_layer_sizes, activation_functions, n_classes)\n",
    "    # Generate random dataset\n",
    "\n",
    "    dataset = MNIST(\n",
    "        str(DATA_PATH / \"mnist\"), train=False, transform=ToTensor(), download=True\n",
    "    )\n",
    "    # Initialize optimizer\n",
    "    optimizer = StochasticGradientDescent(network.parameters(), learning_rate=learning_rate)\n",
    "    # Train network\n",
    "    with torch.no_grad():\n",
    "        losses, accuracies = train_feed_forward_network(network, dataset, optimizer, epochs=epochs,\n",
    "                                                        batch_size=batch_size)\n",
    "    # Plot results\n",
    "    plot_loss_and_accuracy(losses, accuracies)\n",
    "\n",
    "\n",
    "train_network_on_mnist_dataset()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3e087ea721c3098"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a88ac0a1c2ae0121"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
