{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path('..') / '..'\n",
    "!pip install -q -r {ROOT_DIR/ 'requirements.txt'}\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import MNIST, VisionDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from activations import relu\n",
    "from devices import Device\n",
    "from networks import FeedForwardNetwork\n",
    "from utils import DATA_PATH"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T01:50:58.550149100Z",
     "start_time": "2023-08-30T01:50:56.193929100Z"
    }
   },
   "id": "829cb43cfed9f79e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training a Neural Network with Backpropagation\n",
    "\n",
    "Backpropagation is like a teacher for neural networks. It helps the network learn from mistakes by making small changes to its internal settings. Here's how it works:\n",
    "\n",
    "1. **Learning from Examples:** We show the network many examples and tell it the correct answers.\n",
    "2. **Making Guesses:** The network tries to guess the answer for each example.\n",
    "3. **Checking Mistakes:** After guessing, we check how far off its guess was from the correct answer.\n",
    "4. **Learning from Mistakes:** Using the mistakes it made, the network fine-tunes its internal settings to guess better next time.\n",
    "5. **Repeat:** We keep showing examples until the network gets good at guessing right.\n",
    "\n",
    "The magic of backpropagation is in step 4, where it figures out which settings to tweak and by how much. This \"tweaking\" is done using a math trick called gradient descent."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d1b614c4ff7149c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Checking if Gradients are Correct: Gradient Checking\n",
    "\n",
    "Imagine you've got a math formula, and you've made some changes to it. You'd want to double-check if your changes were right. That's what gradient checking does for neural networks.\n",
    "\n",
    "In simple terms, gradient checking compares two methods of finding gradients (slopes). One method uses the standard backpropagation technique. The other uses a quick-and-dirty method called \"finite difference approximation.\" If both methods give similar answers, we can be pretty sure our backpropagation is set up correctly.\n",
    "\n",
    "Here's a simple way to do gradient checking:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a079efc81c08a434"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from loss import cross_entropy\n",
    "\n",
    "\n",
    "def check_gradients(epsilon: float = 1e-6):\n",
    "    # Disable tracking computations\n",
    "    with torch.no_grad():\n",
    "        # Set some basics and random data\n",
    "        samples = 100\n",
    "        input_size = 300\n",
    "        output_classes = 10\n",
    "        # Set up a basic neural network\n",
    "        network = FeedForwardNetwork(input_size, [100, 200], [sigmoid, relu], output_classes)\n",
    "        parameters = list(network.parameters())\n",
    "        # Random input\n",
    "        input_data = torch.randn(samples, input_size)\n",
    "        # Make random target labels\n",
    "        labels = torch.zeros(samples, output_classes)\n",
    "        targets = torch.randint(0, output_classes, (samples,))\n",
    "        labels[torch.arange(samples), targets] = 1\n",
    "        for param in parameters:\n",
    "            # Check the loss when we reduce the parameter a tiny bit\n",
    "            param -= epsilon\n",
    "            pred_minus = network(input_data)\n",
    "            loss_minus = cross_entropy(pred_minus, labels)\n",
    "            # Check the loss when we increase the parameter a tiny bit\n",
    "            param += 2 * epsilon\n",
    "            pred_plus = network(input_data)\n",
    "            loss_plus = cross_entropy(pred_plus, labels)\n",
    "            # Quick-and-dirty gradient calculation\n",
    "            estimated_gradient = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            # Bring parameter back to original\n",
    "            param -= epsilon\n",
    "            # Get the actual gradient using backpropagation\n",
    "            pred = network(input_data)\n",
    "            network.backward(input_data, labels, pred)\n",
    "            # See how different the two gradients are\n",
    "            difference = torch.abs(estimated_gradient - torch.mean(param.grad))\n",
    "            print(f\"Difference between estimated and real gradient: {difference}\")\n",
    "\n",
    "\n",
    "# Run our gradient check\n",
    "check_gradients()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T01:44:13.109686200Z"
    }
   },
   "id": "23784c7957683169"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training the Model\n",
    "\n",
    "Now that we've verified that our gradients are correct, we can train our model. We'll use a RandomDataset. This dataset generates random data and labels on the fly. It's useful for testing and debugging."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110bea0705b2945b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(losses: list[float], accuracies: list[float]):\n",
    "    \"\"\"Plot the loss and accuracy of the model during training\"\"\"\n",
    "    fig_loss = pyplot.figure(1)\n",
    "    loss_ax = fig_loss.add_subplot(111)\n",
    "    loss_ax.set_title(\"Loss\")\n",
    "    loss_ax.set_xlabel(\"epochs\")\n",
    "    loss_ax.set_ylabel(\"loss\")\n",
    "    loss_ax.plot(losses, c=\"r\")\n",
    "\n",
    "    fig_accuracy = pyplot.figure(2)\n",
    "    accuracy_ax = fig_accuracy.add_subplot(111)\n",
    "    accuracy_ax.set_title(\"Accuracy\")\n",
    "    accuracy_ax.set_xlabel(\"epochs\")\n",
    "    accuracy_ax.set_ylabel(\"acc\")\n",
    "    accuracy_ax.plot(accuracies, c=\"b\")\n",
    "    pyplot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T01:44:13.113546700Z"
    }
   },
   "id": "df04e4e0e7706d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import SizedDataset\n",
    "\n",
    "\n",
    "def convert_to_one_hot(tensor: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert tensor to one-hot encoding based on provided labels.\n",
    "    \"\"\"\n",
    "    one_hot = torch.zeros_like(tensor)\n",
    "    one_hot[torch.arange(tensor.size(0)), labels] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def train_one_batch(network: FeedForwardNetwork, optimizer: StochasticGradientDescent, x: torch.Tensor,\n",
    "                    y: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Train the network on a single batch of data.\n",
    "    \"\"\"\n",
    "    y_pred = network(x)\n",
    "    y_onehot = convert_to_one_hot(y_pred, y)\n",
    "    network.backward(x, y_onehot, y_pred)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def report_progress(epoch: int, accuracy: float, loss: float, avg_time: float):\n",
    "    \"\"\"\n",
    "    Report training progress.\n",
    "    \"\"\"\n",
    "    print(f\"\\rEpoch:{epoch:03d} Accuracy:{accuracy:.2f}% Loss:{loss:.4f} Time/epoch:{avg_time:.3f}s\", end='')\n",
    "\n",
    "\n",
    "def calculate_average_time(previous_avg: float, current_time: float, epoch: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average time taken per epoch.\n",
    "    \"\"\"\n",
    "    return (previous_avg * (epoch - 1) + current_time) / epoch\n",
    "\n",
    "\n",
    "def train_feed_forward_network(\n",
    "        network: FeedForwardNetwork,\n",
    "        dataset: SizedDataset | VisionDataset,\n",
    "        optimizer: StochasticGradientDescent,\n",
    "        epochs: int = 1,\n",
    "        batch_size: int = 1,\n",
    "        reports_every: int = 1,\n",
    "        device=Device.CPU\n",
    ") -> tuple[list[float], list[float]]:\n",
    "    network.to(device)\n",
    "    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    dataset_size = len(dataset)\n",
    "    average_time_per_epoch = 0\n",
    "    losses, accuracies = [], []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = timer()\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.view(x.size(0), -1).float().to(device), y.to(device)\n",
    "            train_one_batch(network, optimizer, x, y)\n",
    "        average_time_per_epoch = calculate_average_time(average_time_per_epoch, timer() - epoch_start_time, epoch)\n",
    "        if epoch % reports_every == 0:\n",
    "            x_all = dataset.data.view(dataset_size, -1).float().to(device)\n",
    "            true_labels = dataset.targets.to(device)\n",
    "            predicted_output = network(x_all).to(device)\n",
    "            onehot_prediction = convert_to_one_hot(predicted_output, true_labels)\n",
    "            loss = cross_entropy(predicted_output, onehot_prediction)\n",
    "            losses.append(loss)\n",
    "            predicted_labels = torch.argmax(predicted_output, dim=1)\n",
    "            accuracy = 100 * (predicted_labels == true_labels).sum().item() / dataset_size\n",
    "            accuracies.append(accuracy)\n",
    "            report_progress(epoch, accuracy, loss, average_time_per_epoch)\n",
    "    return losses, accuracies\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T01:44:13.115538500Z"
    }
   },
   "id": "2a7fe3dc68f48367"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_network_on_dataset(dataset_class):\n",
    "    # Hyperparameters\n",
    "    n_samples = 2000\n",
    "    n_features = 300\n",
    "    n_classes = 10\n",
    "    hidden_layer_sizes = [300, 400]\n",
    "    activation_functions = [celu, relu]\n",
    "    activation_function_parameters = [float(n_classes), None]\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "\n",
    "    # Initialize network\n",
    "    network = FeedForwardNetwork(n_features, hidden_layer_sizes, activation_functions,\n",
    "                                 n_classes, activation_function_parameters)\n",
    "\n",
    "    # Generate dataset based on the provided dataset class\n",
    "    dataset = dataset_class(n_samples, n_features, n_classes)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = StochasticGradientDescent(network.parameters(), learning_rate=learning_rate)\n",
    "\n",
    "    # Train network\n",
    "    with torch.no_grad():\n",
    "        losses, accuracies = train_feed_forward_network(network, dataset, optimizer,\n",
    "                                                        epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Plot results\n",
    "    plot_loss_and_accuracy(losses, accuracies)\n",
    "\n",
    "\n",
    "train_network_on_dataset(RandomUniformDataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T01:44:13.116541400Z"
    }
   },
   "id": "c1825dedf74b9dcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import BernoulliDataset\n",
    "\n",
    "train_network_on_dataset(BernoulliDataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T01:44:13.118540500Z"
    }
   },
   "id": "30f1e23843cb3011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import RandomNormalDataset\n",
    "\n",
    "train_network_on_dataset(RandomNormalDataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T01:44:13.119540300Z"
    }
   },
   "id": "570766839a445d6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training the Model on MNIST\n",
    "\n",
    "Having introduced the MNIST dataset and our chosen training methodology, let's delve into the specifics of putting them into action."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41ce62e0bfe8d1cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_network_on_mnist_dataset():\n",
    "    # Hyperparameters\n",
    "    n_features = 784\n",
    "    n_classes = 10\n",
    "    hidden_layer_sizes = [512, 1024, 128]\n",
    "    activation_functions = [relu, relu, relu]\n",
    "    learning_rate = 1e-5\n",
    "    epochs = 30\n",
    "    batch_size = 32\n",
    "    # Initialize network\n",
    "    network = FeedForwardNetwork(n_features, hidden_layer_sizes, activation_functions, n_classes)\n",
    "    # Generate random dataset\n",
    "\n",
    "    dataset = MNIST(\n",
    "        str(DATA_PATH / \"mnist\"), train=False, transform=ToTensor(), download=True\n",
    "    )\n",
    "    # Initialize optimizer\n",
    "    optimizer = StochasticGradientDescent(network.parameters(), learning_rate=learning_rate)\n",
    "    # Train network\n",
    "    with torch.no_grad():\n",
    "        losses, accuracies = train_feed_forward_network(network, dataset, optimizer, epochs=epochs,\n",
    "                                                        batch_size=batch_size)\n",
    "    # Plot results\n",
    "    plot_loss_and_accuracy(losses, accuracies)\n",
    "\n",
    "\n",
    "train_network_on_mnist_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T01:44:13.120541200Z"
    }
   },
   "id": "b3e087ea721c3098"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T01:44:13.121542Z"
    }
   },
   "id": "a88ac0a1c2ae0121"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
